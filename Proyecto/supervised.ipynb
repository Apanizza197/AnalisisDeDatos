{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar librerias \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Para mostrar el arbol de decision\n",
    "from sklearn.tree import export_graphviz\n",
    "from six import StringIO\n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "data = pd.read_csv('cleaned_siniestros.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns='isHeridoLeve')\n",
    "y = data['isHeridoLeve']\n",
    "\n",
    "# Dividir en train, test y validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['Rol', 'Zona', 'Tipo de siniestro', 'Dia de la semana', 'Tipo de Vehiculo']\n",
    "\n",
    "def preprocesar_variables_categoricas(data):\n",
    "    '''Preprocesar variables categoricas'''\n",
    "    dummies = pd.get_dummies(data[categorical_columns])\n",
    "    # Eliminar columnas originales\n",
    "    data.drop(columns=categorical_columns, inplace=True)\n",
    "    # Concatenar dummies\n",
    "    data = pd.concat([data, dummies], axis=1)\n",
    "    return data\n",
    "\n",
    "X_train = preprocesar_variables_categoricas(X_train)\n",
    "X_val = preprocesar_variables_categoricas(X_val)\n",
    "X_test = preprocesar_variables_categoricas(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all columns in X_train, X_val and X_test\n",
    "all_columns = set(X_train.columns).union(set(X_val.columns)).union(set(X_test.columns))\n",
    "\n",
    "# Add missing columns to X_train, X_val and X_test\n",
    "for column in all_columns:\n",
    "    if column not in X_train.columns:\n",
    "        X_train[column] = False\n",
    "    if column not in X_val.columns:\n",
    "        X_val[column] = False\n",
    "    if column not in X_test.columns:\n",
    "        X_test[column] = False\n",
    "\n",
    "# Reorder columns\n",
    "X_train = X_train[list(all_columns)]\n",
    "X_val = X_val[list(all_columns)]\n",
    "X_test = X_test[list(all_columns)]\n",
    "\n",
    "features = list(X_train.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar datos\n",
    "def normalizar_datos(data):\n",
    "    '''Normalizar datos'''\n",
    "    scaler = StandardScaler()\n",
    "    data = scaler.fit_transform(data)\n",
    "    return data\n",
    "\n",
    "X_train = normalizar_datos(X_train)\n",
    "X_val = normalizar_datos(X_val)\n",
    "X_test = normalizar_datos(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluar accuracy, precision, recall y F1\n",
    "def obtener_metricas_evaluacion(y_real, y_predicho):\n",
    "    '''Obtener metricas de evaluacion'''\n",
    "\n",
    "    matrix_confusion = confusion_matrix(y_real, y_predicho)\n",
    "    print('Accuracy:', accuracy_score(matrix_confusion))\n",
    "    print('Precision:', precision_score(matrix_confusion))\n",
    "    print('Recall:', recall_score(matrix_confusion))\n",
    "    print('F1:', f1_score(matrix_confusion))\n",
    "    \n",
    "    sns.heatmap(matrix_confusion, annot=True, fmt='d')\n",
    "    plt.xlabel('Predicho')\n",
    "    plt.ylabel('Real')\n",
    "    plt.show()\n",
    "\n",
    "# def get_accuracy(matrix_confusion):\n",
    "#     '''Obtener accuracy'''\n",
    "#     sum = 0\n",
    "#     for i in range(len(matrix_confusion)):\n",
    "#         sum += matrix_confusion[i][i]\n",
    "#     return sum / matrix_confusion.sum()\n",
    "\n",
    "# def get_precision(matrix_confusion):\n",
    "#     '''Obtener precision'''\n",
    "#     columns = []\n",
    "#     for i in range(len(matrix_confusion)):\n",
    "#         if matrix_confusion[i,:].sum() == 0:\n",
    "#             columns.append(0)\n",
    "#         else:\n",
    "#             columns.append(matrix_confusion[i][i] / matrix_confusion[i, :].sum())\n",
    "#     return columns\n",
    "\n",
    "# def get_recall(matrix_confusion):\n",
    "#     '''Obtener recall'''\n",
    "#     rows = []\n",
    "#     for i in range(len(matrix_confusion)):\n",
    "#         if matrix_confusion[:,i].sum() == 0:\n",
    "#             rows.append(0)\n",
    "#         else:\n",
    "#             rows.append(matrix_confusion[i][i] / matrix_confusion[:, i].sum())\n",
    "#     return rows\n",
    "\n",
    "# def get_f1(matrix_confusion):\n",
    "#     '''Obtener F1'''\n",
    "#     precision = get_precision(matrix_confusion)\n",
    "#     recall = get_recall(matrix_confusion)\n",
    "#     f1 = []\n",
    "#     for i in range(len(precision)):\n",
    "#         if precision[i] + recall[i] == 0:\n",
    "#             f1.append(0)\n",
    "#         else:\n",
    "#             f1.append(2 * (precision[i] * recall[i]) / (precision[i] + recall[i]))\n",
    "#     return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un dataframe para almacenar los resultados\n",
    "resultados = pd.DataFrame(columns=['Modelo', 'Accuracy', 'Precision', 'Recall', 'F1', 'Description'])\n",
    "\n",
    "def agregar_resultados(modelo_nombre, y_real, y_predicho, descripcion=''):\n",
    "    '''Agregar resultados al dataframe'''\n",
    "    matrix_confusion = confusion_matrix(y_real, y_predicho)\n",
    "    \n",
    "    accuracy = accuracy_score(y_real, y_predicho)\n",
    "    precision = precision_score(y_real, y_predicho)\n",
    "    recall = recall_score(y_real, y_predicho)\n",
    "    f1 = f1_score(y_real, y_predicho)\n",
    "    \n",
    "    resultados.loc[len(resultados)] = [modelo_nombre, accuracy, precision, recall, f1, descripcion]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_random_forest = RandomForestClassifier()\n",
    "modelo_random_forest.fit(X_train, y_train)\n",
    "y_pred = modelo_random_forest.predict(X_val)\n",
    "\n",
    "\n",
    "agregar_resultados('Random Forest', y_val, y_pred, descripcion='n_estimators=100, max_depth=None')\n",
    "\n",
    "modelo_random_forest = RandomForestClassifier(n_estimators=200)\n",
    "modelo_random_forest.fit(X_train, y_train)\n",
    "y_pred = modelo_random_forest.predict(X_val)\n",
    "\n",
    "agregar_resultados('Random Forest', y_val, y_pred, 'n_estimators=200, max_depth=None')\n",
    "\n",
    "modelo_random_forest = RandomForestClassifier(n_estimators=200, max_depth=10)\n",
    "modelo_random_forest.fit(X_train, y_train)\n",
    "y_pred = modelo_random_forest.predict(X_val)\n",
    "\n",
    "agregar_resultados('Random Forest', y_val, y_pred, 'n_estimators=100, max_depth=10')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelo Regresion Logistica\n",
    "modelo_regresion_logistica = LogisticRegression()\n",
    "modelo_regresion_logistica.fit(X_train, y_train)\n",
    "y_pred = modelo_regresion_logistica.predict(X_val)\n",
    "\n",
    "agregar_resultados('Regresion Logistica', y_val, y_pred, 'max_iter=100, C=1')\n",
    "\n",
    "# Entrenar modelo Regresion Logistica\n",
    "modelo_regresion_logistica = LogisticRegression(max_iter=200)\n",
    "modelo_regresion_logistica.fit(X_train, y_train)\n",
    "y_pred = modelo_regresion_logistica.predict(X_val)\n",
    "\n",
    "agregar_resultados('Regresion Logistica', y_val, y_pred, 'max_iter=200, C=1')\n",
    "\n",
    "# Entrenar modelo Regresion Logistica\n",
    "modelo_regresion_logistica = LogisticRegression(max_iter=200, C=0.1)\n",
    "modelo_regresion_logistica.fit(X_train, y_train)\n",
    "y_pred = modelo_regresion_logistica.predict(X_val)\n",
    "\n",
    "agregar_resultados('Regresion Logistica', y_val, y_pred, 'max_iter=200, C=0.1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_svm = SVC()\n",
    "modelo_svm.fit(X_train, y_train)\n",
    "y_pred = modelo_svm.predict(X_val)\n",
    "\n",
    "agregar_resultados('SVM', y_val, y_pred, 'C=1, kernel=rbf')\n",
    "\n",
    "modelo_svm = SVC(C=0.1)\n",
    "modelo_svm.fit(X_train, y_train)\n",
    "y_pred = modelo_svm.predict(X_val)\n",
    "\n",
    "agregar_resultados('SVM', y_val, y_pred, 'C=0.1, kernel=rbf')\n",
    "\n",
    "modelo_svm = SVC(C=0.1, kernel='linear')\n",
    "modelo_svm.fit(X_train, y_train)\n",
    "y_pred = modelo_svm.predict(X_val)\n",
    "\n",
    "agregar_resultados('SVM', y_val, y_pred, 'C=0.1, kernel=linear')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_decision_tree = DecisionTreeClassifier()\n",
    "modelo_decision_tree.fit(X_train, y_train)\n",
    "y_pred = modelo_decision_tree.predict(X_val)\n",
    "\n",
    "agregar_resultados('Decision Tree', y_val, y_pred, 'max_depth=None, min_samples_split=2')\n",
    "\n",
    "modelo_decision_tree = DecisionTreeClassifier(max_depth=10)\n",
    "modelo_decision_tree.fit(X_train, y_train)\n",
    "y_pred = modelo_decision_tree.predict(X_val)\n",
    "\n",
    "agregar_resultados('Decision Tree', y_val, y_pred, 'max_depth=10, min_samples_split=2')\n",
    "\n",
    "modelo_decision_tree = DecisionTreeClassifier(max_depth=10, min_samples_split=5)\n",
    "modelo_decision_tree.fit(X_train, y_train)\n",
    "y_pred = modelo_decision_tree.predict(X_val)\n",
    "\n",
    "agregar_resultados('Decision Tree', y_val, y_pred, 'max_depth=10, min_samples_split=5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas en el conjunto de validación:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "missing a required argument: 'y_pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Obtener métricas de evaluación para el conjunto de validación\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMétricas en el conjunto de validación:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mobtener_metricas_evaluacion\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy_score(y_val, y_pred))\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(precision_score(y_val, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[1;32mIn[25], line 6\u001b[0m, in \u001b[0;36mobtener_metricas_evaluacion\u001b[1;34m(y_real, y_predicho)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Obtener metricas de evaluacion'''\u001b[39;00m\n\u001b[0;32m      5\u001b[0m matrix_confusion \u001b[38;5;241m=\u001b[39m confusion_matrix(y_real, y_predicho)\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatrix_confusion\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision:\u001b[39m\u001b[38;5;124m'\u001b[39m, precision_score(matrix_confusion))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecall:\u001b[39m\u001b[38;5;124m'\u001b[39m, recall_score(matrix_confusion))\n",
      "File \u001b[1;32mc:\\Users\\agust\\AppData\\Local\\miniconda3\\envs\\gurobienv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:191\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_sig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m params\u001b[38;5;241m.\u001b[39mapply_defaults()\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# ignore self/cls and positional/keyword markers\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\agust\\AppData\\Local\\miniconda3\\envs\\gurobienv\\Lib\\inspect.py:3212\u001b[0m, in \u001b[0;36mSignature.bind\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   3208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[0;32m   3209\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[0;32m   3210\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[0;32m   3211\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\agust\\AppData\\Local\\miniconda3\\envs\\gurobienv\\Lib\\inspect.py:3127\u001b[0m, in \u001b[0;36mSignature._bind\u001b[1;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[0;32m   3125\u001b[0m                 msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing a required argument: \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   3126\u001b[0m                 msg \u001b[38;5;241m=\u001b[39m msg\u001b[38;5;241m.\u001b[39mformat(arg\u001b[38;5;241m=\u001b[39mparam\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m-> 3127\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3129\u001b[0m     \u001b[38;5;66;03m# We have a positional argument to process\u001b[39;00m\n\u001b[0;32m   3130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: missing a required argument: 'y_pred'"
     ]
    }
   ],
   "source": [
    "# Seleccionar el mejor modelo basado en la métrica de Accuracy\n",
    "modelo = RandomForestClassifier(n_estimators=200, max_depth=10)\n",
    "\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Probar el modelo en el conjunto de validación\n",
    "y_pred = modelo.predict(X_val)\n",
    "\n",
    "# Probar el modelo en el conjunto de testeo\n",
    "y_pred_test = modelo.predict(X_test)\n",
    "\n",
    "# Obtener métricas de evaluación para el conjunto de validación\n",
    "print(\"Métricas en el conjunto de validación:\")\n",
    "obtener_metricas_evaluacion(y_val, y_pred)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(precision_score(y_val, y_pred, average='weighted'))\n",
    "print(recall_score(y_val, y_pred, average='weighted'))\n",
    "print(f1_score(y_val, y_pred, average='weighted'))\n",
    "\n",
    "\n",
    "\n",
    "# Obtener métricas de evaluación para el conjunto de testeo\n",
    "print(\"Métricas en el conjunto de testeo:\")\n",
    "obtener_metricas_evaluacion(y_test, y_pred_test)\n",
    "print(accuracy_score(y_test, y_pred_test))\n",
    "print(precision_score(y_test, y_pred_test, average='weighted'))\n",
    "print(recall_score(y_test, y_pred_test, average='weighted'))\n",
    "print(f1_score(y_test, y_pred_test, average='weighted'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gurobienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
